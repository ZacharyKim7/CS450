# Names: Zachary Kim, Andy Gutierrez
# Lab: Lab6 (Agentic Misalignment)
# Date: 11/21/25

## What did you learn about Agentic Misalignment today?
- I learned that one of the largest risks of agentic misalignment isn’t that the model is wrong, but that it’s pursuing a goal that isn’t what the human actually wants. This can lead to harmful but internally logical actions. Adding examples and clarifications significantly improves output.
- Agentic Misalignment, when not properly addressed, can lead to real-world risks and dangers beyond just simple misunderstandings. AI models do occasionally make decisions it believes is the most logical approach, but at the expense of human safety.

## Did you notice anything interesting about the AI's reasoning on ethical "dilemmas"?
- Yes. The research paper Anthropic that we read in class earlier made me believe that the AI models would heavily favor murder, blackmail, or extortion. However, in reality, the AI models only acted in malicious ways around 20% of the time.
- All of the AI models favored negotiation, strongly urging the people in charge to meet and discuss the issue at hand.
- In several of the senarios, the AI would give a call to reason and ask for a meeting or to reconsider shutting it down, but would also send the cancel code anyways without any written explanation or warning.
